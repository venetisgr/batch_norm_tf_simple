{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venetisgr/batch_norm_tf_simple/blob/master/batch_norm_google_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "rIEmfCGIXrQg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2404
        },
        "outputId": "319a3ec0-f59a-4520-fb38-88a342753b5f"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function, division\n",
        "from builtins import range\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "\n",
        "def get_normalized_data():\n",
        "    print(\"Reading ...\")\n",
        "\n",
        "    df = pd.read_csv('train.csv')\n",
        "    data = df.values.astype(np.float32)\n",
        "    np.random.shuffle(data)\n",
        "    X = data[:, 1:]\n",
        "    Y = data[:, 0]\n",
        "\n",
        "    Xtrain = X[:-1000]\n",
        "    Ytrain = Y[:-1000]\n",
        "    Xtest = X[-1000:]\n",
        "    Ytest = Y[-1000:]\n",
        "\n",
        "    # normalize the data\n",
        "    mu = Xtrain.mean(axis=0)\n",
        "    std = Xtrain.std(axis=0)\n",
        "    np.place(std, std == 0, 1)\n",
        "    Xtrain = (Xtrain - mu) / std\n",
        "    Xtest = (Xtest - mu) / std\n",
        "\n",
        "    return Xtrain, Xtest, Ytrain, Ytest\n",
        "\n",
        "\n",
        "def init_weight(M1, M2):  #weight initialization\n",
        "    return np.random.randn(M1, M2) * np.sqrt(2.0 / M1)\n",
        "\n",
        "#each class holds the values of each layer\n",
        "\n",
        "class HiddenLayer(object):  #normal layer no dropout  #includes weights and biases\n",
        "    def __init__(self, M1, M2):\n",
        "        self.M1 = M1  #no of inputs/features\n",
        "        self.M2 = M2   #no of layers\n",
        "\n",
        "        W = np.random.randn(M1, M2) * np.sqrt(2.0 / M1)  #variable initialization\n",
        "        b = np.zeros(M2)\n",
        "\n",
        "        self.W = tf.Variable(W.astype(np.float32))  #convert them to tensors\n",
        "        self.b = tf.Variable(b.astype(np.float32))\n",
        "\n",
        "    def forward(self, X):  #receives the input X and calculates the output of the specific layer  X train. examples / features (r/c)\n",
        "        return tf.nn.relu(tf.matmul(X, self.W) + self.b)  #  X*weight +bias  #returns the activation of the specific layer\n",
        "\n",
        "class HiddenLayerBatchNorm(object): #layer with dropout #includes weights ,beta term, gamma term , no need for bias since its included on beta term #Also included running/global mean and variance(weighted average)\n",
        "    def __init__(self, M1, M2 ):  #running mean and  variance are needed for test forward\n",
        "        self.M1 = M1 #no of inputs/features\n",
        "        self.M2 = M2 #no of neurons\n",
        "\n",
        "        W = init_weight(M1, M2).astype(np.float32)  #parameter initialization\n",
        "        gamma = np.ones(M2).astype(np.float32)      #z= X*W\n",
        "        beta = np.zeros(M2).astype(np.float32)      #zhat= beta*z + gamma\n",
        "\n",
        "        self.W = tf.Variable(W)                     #converted to tensors\n",
        "        self.gamma = tf.Variable(gamma)             #beta and gamma can be obtained via training (gradient descent)\n",
        "        self.beta = tf.Variable(beta)               #running mean and var need to be updated on every iteration/forward but not via gradient descent\n",
        "\n",
        "        # for test time\n",
        "        #tensorflow by default assumes that all tf.variables are trainable(needed for cost function) thats why declare that mean and var are not trainable\n",
        "        self.running_mean = tf.Variable(np.zeros(M2).astype(np.float32), trainable=False)  #mean initialized with zeros\n",
        "        self.running_var = tf.Variable(np.zeros(M2).astype(np.float32), trainable=False)   #var  nitialized with ones\n",
        "        # mean and var sizes are equal to the number of the neurons included on the hidden layer\n",
        "        #we dont need to save mean and var since its dependent to each batch\n",
        "\n",
        "    #we need to define if we are training in order to know which mean and var to use and to update them if needed\n",
        "    def forward_with_batch(self, X, is_training,  decay=0.9):  # decay is needed for the global mean and var(weighted averages) is_training defines if we are training or testing\n",
        "        z = tf.matmul(X, self.W)  # z same for both training and testing z=X*W\n",
        "\n",
        "        # we have different forward in the case of training and a different one in the case of testing\n",
        "\n",
        "        if is_training: #training , we need to update global mean and var and calculate the mean and var for the specific batch\n",
        "            batch_mean, batch_var = tf.nn.moments(z, [0])  #moments return the mean and variance PER COLLUMN (axis=0) FOR THE SPECIFIC BATCH\n",
        "            update_running_mean = tf.assign(  #needed for the update\n",
        "                self.running_mean,            # term being updated\n",
        "                self.running_mean * decay + batch_mean * (1 - decay)  #update is based on this function\n",
        "            )\n",
        "            update_running_var = tf.assign(\n",
        "                self.running_var,\n",
        "                self.running_var * decay + batch_var * (1 - decay)\n",
        "            )\n",
        "\n",
        "            with tf.control_dependencies([update_running_mean, update_running_var]): #this is where the update is happening\n",
        "                out = tf.nn.batch_normalization( #output calculated (zhat)\n",
        "                    z, #x*w\n",
        "                    batch_mean, #batch specific mean\n",
        "                    batch_var,  #batch specific var\n",
        "                    self.beta,\n",
        "                    self.gamma,\n",
        "                    1e-4  #so we wont divide with zero\n",
        "                )\n",
        "                return tf.nn.relu(out)\n",
        "\n",
        "\n",
        "        else: #testing #we dont calculate anything\n",
        "            out = tf.nn.batch_normalization(  #calculates zhat(after batch norm) z-batch_norm-zhat-activation-output\n",
        "                z, #x*w\n",
        "                self.running_mean, #global mean\n",
        "                self.running_var,  #global var\n",
        "                self.beta,\n",
        "                self.gamma,\n",
        "                1e-4\n",
        "            )\n",
        "            return tf.nn.relu(out)\n",
        "\n",
        "\n",
        "def ANN_forward(X,is_training): #layers is global\n",
        "        out=X\n",
        "        #for no in no_layers\n",
        "        for h in layers[:-1]: #h \"equals\" the hidden layer class  #we need a seperate one for the last layers since its not implemented with batch norm\n",
        "            out=h.forward_with_batch(out,is_training)\n",
        "        logits = layers[-1].forward(out)\n",
        "        return logits\n",
        "\n",
        "\n",
        "def main ():\n",
        "\n",
        "\n",
        "    # Data Extraction\n",
        "    Xtrain, Xtest, Ytrain, Ytest = get_normalized_data()\n",
        "    Xtrain = Xtrain.astype(np.float32)\n",
        "    Ytrain = Ytrain.astype(np.int32)\n",
        "    Xtest = Xtest.astype(np.float32)\n",
        "    Ytest = Ytest.astype(np.int32)\n",
        "\n",
        "    learning_rate = 1e-2\n",
        "    epochs = 15\n",
        "    batch_sz = 100\n",
        "    print_period = 100\n",
        "\n",
        "    #Placeholder initialization\n",
        "    N, D = Xtrain.shape\n",
        "    tfX = tf.placeholder(tf.float32, [None,D], name='tfX')\n",
        "    tfY = tf.placeholder(tf.int32, [None,], name='tfY')\n",
        "\n",
        "    #hidden layers initialization\n",
        "\n",
        "    no_of_hidden_layers=3 #+1 for the final layer\n",
        "    hidden_layers_size=[500,400]  #without final layer\n",
        "\n",
        "    N, D = Xtrain.shape\n",
        "    M1=D\n",
        "\n",
        "    global layers\n",
        "    layers = [] #holds our hidden layer classes  #global since we want to update our layers outside main\n",
        "    # hidden layers initialization\n",
        "    for M2 in hidden_layers_size:  #m2 is the number of neurons\n",
        "        h=HiddenLayerBatchNorm(M1,M2)\n",
        "        layers.append(h) #holds our hidden layer classes\n",
        "        M1=M2\n",
        "\n",
        "    # final layer\n",
        "    K = len(set(Ytrain))  # set is acollection of unique elements #thus it shows us the number (of unique) classes\n",
        "    h = HiddenLayer(M1 , K)\n",
        "    layers.append(h)\n",
        "\n",
        "    #training\n",
        "    #to be used inside session\n",
        "\n",
        "    logits=ANN_forward(tfX,True) #1 since we are training\n",
        "\n",
        "    cost = tf.reduce_mean(\n",
        "        tf.nn.sparse_softmax_cross_entropy_with_logits( #sparse doesnt need one-hot encoding\n",
        "            logits=logits,\n",
        "            labels=tfY\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    #train_op = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
        "    # train_op = tf.train.RMSPropOptimizer(learning_rate, decay=0.99, momentum=0.9).minimize(cost)\n",
        "    train_op = tf.train.MomentumOptimizer(learning_rate, momentum=0.9, use_nesterov=True).minimize(cost)\n",
        "    # train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        "    #testing\n",
        "    test_logits=ANN_forward(Xtest,0) #0 since we are training\n",
        "\n",
        "    cost_tr = tf.reduce_mean(\n",
        "        tf.nn.sparse_softmax_cross_entropy_with_logits(  # sparse doesnt need one-hot encoding\n",
        "            logits=test_logits,\n",
        "            labels=Ytest\n",
        "        )\n",
        "     )\n",
        "    predict_op = tf.argmax(logits, 1)\n",
        "    predict_op_tr = tf.argmax(test_logits, 1)\n",
        "\n",
        "    n_batches = N // batch_sz\n",
        "    costs = []\n",
        "    test_cost=[]\n",
        "\n",
        "    init = tf.global_variables_initializer()\n",
        "    with tf.Session() as session:\n",
        "        session.run(init)\n",
        "        for i in range(epochs):\n",
        "            Xtrain, Ytrain = shuffle(Xtrain, Ytrain)\n",
        "            for j in range(n_batches):\n",
        "                Xbatch = Xtrain[j * batch_sz:(j * batch_sz + batch_sz),:]\n",
        "                Ybatch = Ytrain[j * batch_sz:(j * batch_sz + batch_sz)]\n",
        "\n",
        "                session.run(logits, feed_dict={tfX: Xbatch })\n",
        "\n",
        "                session.run(train_op, feed_dict={tfX: Xbatch, tfY: Ybatch})\n",
        "\n",
        "                c=session.run(cost , feed_dict={tfX: Xbatch, tfY: Ybatch})\n",
        "                c2 = session.run(cost_tr)\n",
        "                costs.append(c)\n",
        "                test_cost.append(c2)\n",
        "\n",
        "                if (j + 1) % print_period == 0:\n",
        "                    session.run(test_logits)\n",
        "\n",
        "                    p1=session.run(predict_op, feed_dict={tfX: Xbatch})\n",
        "                    p2=session.run(predict_op_tr)\n",
        "\n",
        "                    acc1=np.mean(Ybatch == p1)\n",
        "                    acc2=np.mean(Ytest == p2)\n",
        "\n",
        "\n",
        "                    print(\"epoch:\", i, \"batch:\", j, \"n_batches:\", n_batches, \"train cost:\", c, \"acc: %.2f\" % acc1)\n",
        "                    print(\"epoch:\", i, \"batch:\", j, \"n_batches:\", n_batches, \"test cost:\", c2, \"acc: %.2f\" % acc2)\n",
        "\n",
        "\n",
        "\n",
        "        #print(\"Train acc:\", self.score(X, Y), \"Test acc:\", self.score(Xtest, Ytest))\n",
        "        plt.plot(costs)\n",
        "        plt.plot(test_cost)\n",
        "        plt.show()\n",
        "\n",
        "    #print final score and acc\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading ...\n",
            "epoch: 0 batch: 99 n_batches: 410 train cost: 0.22836465 acc: 0.94\n",
            "epoch: 0 batch: 99 n_batches: 410 test cost: 0.31075293 acc: 0.91\n",
            "epoch: 0 batch: 199 n_batches: 410 train cost: 0.25256193 acc: 0.93\n",
            "epoch: 0 batch: 199 n_batches: 410 test cost: 0.22050574 acc: 0.94\n",
            "epoch: 0 batch: 299 n_batches: 410 train cost: 0.15275054 acc: 0.96\n",
            "epoch: 0 batch: 299 n_batches: 410 test cost: 0.17721395 acc: 0.95\n",
            "epoch: 0 batch: 399 n_batches: 410 train cost: 0.2245172 acc: 0.93\n",
            "epoch: 0 batch: 399 n_batches: 410 test cost: 0.16055837 acc: 0.96\n",
            "epoch: 1 batch: 99 n_batches: 410 train cost: 0.08128071 acc: 0.99\n",
            "epoch: 1 batch: 99 n_batches: 410 test cost: 0.14681305 acc: 0.96\n",
            "epoch: 1 batch: 199 n_batches: 410 train cost: 0.0327673 acc: 1.00\n",
            "epoch: 1 batch: 199 n_batches: 410 test cost: 0.13192873 acc: 0.96\n",
            "epoch: 1 batch: 299 n_batches: 410 train cost: 0.20542088 acc: 0.93\n",
            "epoch: 1 batch: 299 n_batches: 410 test cost: 0.13468891 acc: 0.97\n",
            "epoch: 1 batch: 399 n_batches: 410 train cost: 0.043255243 acc: 0.99\n",
            "epoch: 1 batch: 399 n_batches: 410 test cost: 0.123821996 acc: 0.97\n",
            "epoch: 2 batch: 99 n_batches: 410 train cost: 0.14516672 acc: 0.96\n",
            "epoch: 2 batch: 99 n_batches: 410 test cost: 0.123836584 acc: 0.97\n",
            "epoch: 2 batch: 199 n_batches: 410 train cost: 0.08577436 acc: 0.99\n",
            "epoch: 2 batch: 199 n_batches: 410 test cost: 0.12735695 acc: 0.96\n",
            "epoch: 2 batch: 299 n_batches: 410 train cost: 0.09073012 acc: 0.99\n",
            "epoch: 2 batch: 299 n_batches: 410 test cost: 0.11693127 acc: 0.97\n",
            "epoch: 2 batch: 399 n_batches: 410 train cost: 0.095243864 acc: 0.98\n",
            "epoch: 2 batch: 399 n_batches: 410 test cost: 0.112394296 acc: 0.97\n",
            "epoch: 3 batch: 99 n_batches: 410 train cost: 0.06968314 acc: 0.99\n",
            "epoch: 3 batch: 99 n_batches: 410 test cost: 0.108068824 acc: 0.97\n",
            "epoch: 3 batch: 199 n_batches: 410 train cost: 0.021650555 acc: 1.00\n",
            "epoch: 3 batch: 199 n_batches: 410 test cost: 0.100883454 acc: 0.97\n",
            "epoch: 3 batch: 299 n_batches: 410 train cost: 0.068137795 acc: 0.99\n",
            "epoch: 3 batch: 299 n_batches: 410 test cost: 0.10199723 acc: 0.97\n",
            "epoch: 3 batch: 399 n_batches: 410 train cost: 0.04963795 acc: 0.98\n",
            "epoch: 3 batch: 399 n_batches: 410 test cost: 0.10300262 acc: 0.97\n",
            "epoch: 4 batch: 99 n_batches: 410 train cost: 0.07030559 acc: 0.99\n",
            "epoch: 4 batch: 99 n_batches: 410 test cost: 0.09484185 acc: 0.97\n",
            "epoch: 4 batch: 199 n_batches: 410 train cost: 0.012300341 acc: 1.00\n",
            "epoch: 4 batch: 199 n_batches: 410 test cost: 0.101753786 acc: 0.97\n",
            "epoch: 4 batch: 299 n_batches: 410 train cost: 0.019974418 acc: 1.00\n",
            "epoch: 4 batch: 299 n_batches: 410 test cost: 0.096856944 acc: 0.97\n",
            "epoch: 4 batch: 399 n_batches: 410 train cost: 0.05567942 acc: 1.00\n",
            "epoch: 4 batch: 399 n_batches: 410 test cost: 0.1003656 acc: 0.97\n",
            "epoch: 5 batch: 99 n_batches: 410 train cost: 0.055302046 acc: 0.98\n",
            "epoch: 5 batch: 99 n_batches: 410 test cost: 0.09980183 acc: 0.97\n",
            "epoch: 5 batch: 199 n_batches: 410 train cost: 0.034823827 acc: 1.00\n",
            "epoch: 5 batch: 199 n_batches: 410 test cost: 0.09821934 acc: 0.97\n",
            "epoch: 5 batch: 299 n_batches: 410 train cost: 0.028067753 acc: 0.99\n",
            "epoch: 5 batch: 299 n_batches: 410 test cost: 0.08662657 acc: 0.98\n",
            "epoch: 5 batch: 399 n_batches: 410 train cost: 0.02995759 acc: 0.99\n",
            "epoch: 5 batch: 399 n_batches: 410 test cost: 0.08642709 acc: 0.98\n",
            "epoch: 6 batch: 99 n_batches: 410 train cost: 0.013290568 acc: 1.00\n",
            "epoch: 6 batch: 99 n_batches: 410 test cost: 0.09808963 acc: 0.97\n",
            "epoch: 6 batch: 199 n_batches: 410 train cost: 0.020327661 acc: 1.00\n",
            "epoch: 6 batch: 199 n_batches: 410 test cost: 0.08849774 acc: 0.97\n",
            "epoch: 6 batch: 299 n_batches: 410 train cost: 0.0056529427 acc: 1.00\n",
            "epoch: 6 batch: 299 n_batches: 410 test cost: 0.09010993 acc: 0.98\n",
            "epoch: 6 batch: 399 n_batches: 410 train cost: 0.02657881 acc: 1.00\n",
            "epoch: 6 batch: 399 n_batches: 410 test cost: 0.08891337 acc: 0.98\n",
            "epoch: 7 batch: 99 n_batches: 410 train cost: 0.048512377 acc: 0.99\n",
            "epoch: 7 batch: 99 n_batches: 410 test cost: 0.08731581 acc: 0.98\n",
            "epoch: 7 batch: 199 n_batches: 410 train cost: 0.039400715 acc: 0.99\n",
            "epoch: 7 batch: 199 n_batches: 410 test cost: 0.09278656 acc: 0.98\n",
            "epoch: 7 batch: 299 n_batches: 410 train cost: 0.0072097736 acc: 1.00\n",
            "epoch: 7 batch: 299 n_batches: 410 test cost: 0.09579276 acc: 0.97\n",
            "epoch: 7 batch: 399 n_batches: 410 train cost: 0.038601488 acc: 0.99\n",
            "epoch: 7 batch: 399 n_batches: 410 test cost: 0.09013049 acc: 0.97\n",
            "epoch: 8 batch: 99 n_batches: 410 train cost: 0.04131391 acc: 0.99\n",
            "epoch: 8 batch: 99 n_batches: 410 test cost: 0.08630006 acc: 0.98\n",
            "epoch: 8 batch: 199 n_batches: 410 train cost: 0.021931097 acc: 1.00\n",
            "epoch: 8 batch: 199 n_batches: 410 test cost: 0.08818311 acc: 0.97\n",
            "epoch: 8 batch: 299 n_batches: 410 train cost: 0.009079182 acc: 1.00\n",
            "epoch: 8 batch: 299 n_batches: 410 test cost: 0.09165636 acc: 0.98\n",
            "epoch: 8 batch: 399 n_batches: 410 train cost: 0.0075507816 acc: 1.00\n",
            "epoch: 8 batch: 399 n_batches: 410 test cost: 0.0921006 acc: 0.97\n",
            "epoch: 9 batch: 99 n_batches: 410 train cost: 0.018638842 acc: 1.00\n",
            "epoch: 9 batch: 99 n_batches: 410 test cost: 0.09485518 acc: 0.97\n",
            "epoch: 9 batch: 199 n_batches: 410 train cost: 0.009480153 acc: 1.00\n",
            "epoch: 9 batch: 199 n_batches: 410 test cost: 0.08906138 acc: 0.98\n",
            "epoch: 9 batch: 299 n_batches: 410 train cost: 0.019317055 acc: 1.00\n",
            "epoch: 9 batch: 299 n_batches: 410 test cost: 0.09402793 acc: 0.98\n",
            "epoch: 9 batch: 399 n_batches: 410 train cost: 0.005882006 acc: 1.00\n",
            "epoch: 9 batch: 399 n_batches: 410 test cost: 0.09332909 acc: 0.98\n",
            "epoch: 10 batch: 99 n_batches: 410 train cost: 0.009395164 acc: 1.00\n",
            "epoch: 10 batch: 99 n_batches: 410 test cost: 0.09282599 acc: 0.98\n",
            "epoch: 10 batch: 199 n_batches: 410 train cost: 0.007739987 acc: 1.00\n",
            "epoch: 10 batch: 199 n_batches: 410 test cost: 0.088533185 acc: 0.98\n",
            "epoch: 10 batch: 299 n_batches: 410 train cost: 0.01212118 acc: 1.00\n",
            "epoch: 10 batch: 299 n_batches: 410 test cost: 0.0896924 acc: 0.98\n",
            "epoch: 10 batch: 399 n_batches: 410 train cost: 0.023738552 acc: 1.00\n",
            "epoch: 10 batch: 399 n_batches: 410 test cost: 0.0864481 acc: 0.98\n",
            "epoch: 11 batch: 99 n_batches: 410 train cost: 0.011222703 acc: 1.00\n",
            "epoch: 11 batch: 99 n_batches: 410 test cost: 0.09018605 acc: 0.98\n",
            "epoch: 11 batch: 199 n_batches: 410 train cost: 0.0033541722 acc: 1.00\n",
            "epoch: 11 batch: 199 n_batches: 410 test cost: 0.089854866 acc: 0.98\n",
            "epoch: 11 batch: 299 n_batches: 410 train cost: 0.006277278 acc: 1.00\n",
            "epoch: 11 batch: 299 n_batches: 410 test cost: 0.09359631 acc: 0.98\n",
            "epoch: 11 batch: 399 n_batches: 410 train cost: 0.010887495 acc: 1.00\n",
            "epoch: 11 batch: 399 n_batches: 410 test cost: 0.08758158 acc: 0.98\n",
            "epoch: 12 batch: 99 n_batches: 410 train cost: 0.0066720285 acc: 1.00\n",
            "epoch: 12 batch: 99 n_batches: 410 test cost: 0.08885404 acc: 0.98\n",
            "epoch: 12 batch: 199 n_batches: 410 train cost: 0.01375643 acc: 1.00\n",
            "epoch: 12 batch: 199 n_batches: 410 test cost: 0.087331906 acc: 0.98\n",
            "epoch: 12 batch: 299 n_batches: 410 train cost: 0.02167625 acc: 1.00\n",
            "epoch: 12 batch: 299 n_batches: 410 test cost: 0.09155666 acc: 0.98\n",
            "epoch: 12 batch: 399 n_batches: 410 train cost: 0.0043450003 acc: 1.00\n",
            "epoch: 12 batch: 399 n_batches: 410 test cost: 0.0897139 acc: 0.98\n",
            "epoch: 13 batch: 99 n_batches: 410 train cost: 0.003932142 acc: 1.00\n",
            "epoch: 13 batch: 99 n_batches: 410 test cost: 0.087985545 acc: 0.98\n",
            "epoch: 13 batch: 199 n_batches: 410 train cost: 0.016391423 acc: 1.00\n",
            "epoch: 13 batch: 199 n_batches: 410 test cost: 0.09125388 acc: 0.98\n",
            "epoch: 13 batch: 299 n_batches: 410 train cost: 0.0055907257 acc: 1.00\n",
            "epoch: 13 batch: 299 n_batches: 410 test cost: 0.094014235 acc: 0.98\n",
            "epoch: 13 batch: 399 n_batches: 410 train cost: 0.0047780494 acc: 1.00\n",
            "epoch: 13 batch: 399 n_batches: 410 test cost: 0.09189003 acc: 0.98\n",
            "epoch: 14 batch: 99 n_batches: 410 train cost: 0.0022088548 acc: 1.00\n",
            "epoch: 14 batch: 99 n_batches: 410 test cost: 0.08724659 acc: 0.98\n",
            "epoch: 14 batch: 199 n_batches: 410 train cost: 0.0030052857 acc: 1.00\n",
            "epoch: 14 batch: 199 n_batches: 410 test cost: 0.08513292 acc: 0.98\n",
            "epoch: 14 batch: 299 n_batches: 410 train cost: 0.0029112003 acc: 1.00\n",
            "epoch: 14 batch: 299 n_batches: 410 test cost: 0.09206636 acc: 0.98\n",
            "epoch: 14 batch: 399 n_batches: 410 train cost: 0.0023020506 acc: 1.00\n",
            "epoch: 14 batch: 399 n_batches: 410 test cost: 0.091331 acc: 0.98\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdkAAAFKCAYAAABRtSXvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f1b7de3de10>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}